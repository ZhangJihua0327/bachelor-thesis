\chapter{总结与分析}
本章对论文工作进行了总结，并展望了未来可能的优化和改进方向。
\section{工作总结}
本文主要目的是验证机器学习，尤其是强化学习在对分布式系统规约的归纳不变式生成领域中的可行性和有效性。

本文在\TLA 的语言平台上，实现了一个基于强化学习的归纳不变式生成系统，
介绍了系统设计的预备知识和理论基础，以RLTLA的系统体系结构的设计和实现。

\TLA 相较于 IVy 更加复杂，其中存在灵活多样的数据结构，同时也支持任意的嵌套来表达规约。
这对开发人员在设计阶段表达系统的行为和状态转移关系提供了很大的便利，但这点对于归纳不变式生成工具的设计并不友好。

实现上，本文依靠于\TLA 源文件和 endive 对于\TLA 解析和人工识别的假设作为输入，基于 try-and-error 的生成思路，
采用强化学习的方式生成候选不变式，通过模型检查器验证候选不变式的正确性，独立性以及当前所有候选不变式析取结果的递归性，最终生成归纳不变式。
这一生成思路和大部分的归纳不变式生成工具类似，但是在实现上，本文寄希望于强化学习以提高生成效率。

本文使用 gymnasium 实现强化学习的算法，并借助 tianshou 提供的强化学习算法，实现完整的强化学习模块。
强化学习模块接受TLC或Apalache的反馈，基于已经给出的假设和\TLA 源文件生成候选不变式，并交给模型检查器验证。
gymnasium 提供了标准的环境接口，是目前十分受欢迎的环境开源工作，方便了强化学习环境的实现，并且可以接入多种强化学习算法。
tianshou 是一个完全基于 Pytorch 的强化学习框架，方便我们使用多种算法对强化学习模块进行训练和比较。

本文使用TLC和Apalache对生成的候选不变式进行验证，检验生成的候选不变式的正确性，独立性和与已有不变式析取结果的递归性，并将结果返回给强化学习模块。
TLC和Apalache没有提供python的接口，本文通过调用命令行的方式调用TLC和Apalache，并通过对命令行结果的解析，获取验证结果。

在测试部分，本文使用 endive 提供的测试用例对系统进行测试，并和endive进行了比较，
验证了强化学习在面向分布式系统规约的归纳不变式生成工作中具有可行性和有效性。

\section{未来展望}
由于时间和能力的限制，本文所实现的系统的性能和实现方式上有许多不足，存在大量的改进空间。

目前，和endive一样，RLTLA还以来于一些人工的输入，即一些人工识别的假设(predicates)
，这些假设的得到是依赖于人脑对于 \TLA 规约理解，尤其是对每个 Action 进行调用时参数类型的理解。
如果要自动化地识别和生成这些假设，也是一个复杂的工作，目前系统还不具备这项能力。
与此同时，人脑的参与可能带来效率的下降和不可预计的错处的出现。
未来需要实现一个功能更加丰富的静态分析工具，以自动提取出 \TLA 规约的语义信息，包括 Action 的参数类型等，
以帮助强化学习模块更好地理解 \TLA 规约和生成合适的候选不变式，或者通过接入大模型的方式，完成归纳不变式的生成。
另一方面，人工的输入也约束了可搜索的空间的大小，在这一条件下，
如同 endive 的通过机械搜索的方式，可能获得比强化学习更快的生成效率。

其次，目前提供给系统可以选择的谓词的范围基于\TLA 已经定义的最高层次的谓词，并没有考虑谓词的子式以及谓词之间的关系。
系统也无法考虑给出的这些谓词表达式之间，以及每个可能的候选不变式之间的关系。
目前系统设计上，一次只生成一个可能的候选不变式，
这两方面因素，导致了对于每一次的生成的候选不变式的检查，都需要调用1-3次模型检查器进行检查，这往往很花时间，导致系统的效率较低。

基于\TLA 的归纳不变式的生成是一个复杂的问题，在这一领域的研究不是十分充足，可以参考和对比的工作较少。
目前，对于基于\TLA 的归纳不变式生成工具还没有统一的测试集合，也没有十分充足的测试用例。
这导致我们一方面很难评估系统的效率，另一方面，也很难提供给强化学习模块足够的训练数据。
在 endive 的测试集合下，一个归纳不变式常常只需要不超过10个子式析取而来。
在这一背景下，强化学习的效率并不理想，常常带来相较于 endive 等工作提供的搜索算法更高的开支和更低的效率。

